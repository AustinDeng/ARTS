<h1 align="center">The Google File System</h1>
<h3 align="center">Sanjay Ghemawat,Howard Gobioff,and Shun-Tak Leung</h3>
<h4 align="right">Translator:Tsand</h4>
<h4 align="right">Begin Time:2019/4/25</h4>
<h4 align="right">End Time:~~~~       </h4>

<br>

## ABSTRACT
> 我们已经设计和实现的谷歌文件系统：一个针对大型数据密集型应用的分布式系统。即使运行在不昂贵的商业硬件上上她也表现出很好的容错性，与此同时她在大量客户端上显示出很高的性能。

> 尽管像先前分布式文件系统那样有很多相同目标，我们的设计源于对我们应用的工作负载，技术环境的观察，从当前和期盼角度，这都体现了与一些早期文件系统模型的标志性区别。这让我们得以重新检查传统的方案和有效的探索不同的设计点。

> 该文件系统成功的满足了我们的存储需求。她被谷歌广泛的用于由我们服务器产生和处理数据的存储平台以及调研及开发那些需要大数据节点的应用。最大的数据簇提供经过超过一千台机器上的上万块硬盘数以万计T字节的存储，也被上前台服务器并发的访问。

> 在这篇论文中，我们描述设计用于分布式应用的文件系统的表面扩展，谈及我们设计的许多方面并且报告理论实验室和现实使用的评测。

## Categories and Subject Descriptors

> D[4]:3--Distributed file systems

## General Terms

> Design ,reliability, performance,measurement

## Keywords

> Fault tolerate,scalability,data storage,clustered storage

## 1. INTRODUCTION

> 我们设计和实现的谷歌文件系统满足谷歌日益快速增长的数据处理需求。GFS拥有之前分布式文件系统的很多目标例如：表现，可扩展性，可靠性和实用性。但是她的设计源于我们对我们应用的工作负载技术环境的关键观察。从当前和预期角度都与之前的文件系统模型有着很大区别。我们已经重新检查传统的选择点并且探索出在设计方面更加实用的方案。

> 首先，组件错误是常态而不是例外。该文件系统由上千台甚至上万台由并不昂贵的商业零件组成并被大量客户端机器并发访问。组件的数量及质量表明有些在指定时间内不工作有些在出现错误时不能恢复。我们已经发现一些由应用bug,操作系统bug,人为错误，以及硬盘损坏，内存，连接，网络和供电引起的问题。因此，持续的监控，错误甄别和自动恢复要添加到系统中。

> 第二点，按照传统标准文件是相当巨大的。大小几个GB的文件是常有的情况。每一个文件都典型的包含许多如web文件的对象。当我们经常和快速增长的包含数百万个对象的许多TBS大小数据集打交道时.即使系统支持这麽做，管理数百万个大约KB级别大小的文件也是不容易的。由于上述原因，设计模型和参数时如IO操作和块大小必须能被重复访问。

> 第三点，大多数文件改变是由增添新的数据而不是重写已有数据引起的。随机写文件是无法实现的。一旦被写入，文件变成只读而且经常是按顺序地。大量数据拥有这些特性。一些可能构成数据分析程序可以访问的大仓库。一些可能是正在运行的应用产生的持续数据流。一些可能是档案数据。一些是由一台机器产生另一台机器同时或稍后及时处理的中间产物。介于这种大文件的访问方式。appending技术成为性能最大化和完整性保证的关键，与此同时在客户端中缓存数据块丧失了她的吸引力。

> 第四点，同步设计应用和文件系统的API接口对增加整个系统的流畅度很有好处。例如，我们在不把巨大的负担增添到应用上的情况下通过减轻GFS的一致性模型来更大程度的简化文件系统。为了多个客户端能同时增添一个文件而不需要他们之间额外的同步我们也引入原子性增添操作。
  多个GFS簇通常被用于不同目的。最大的一个拥有超过1000个存储节点，超过300TB硬盘存储。被远程机器上的成百上千台客服端持续的访问。
  
#  2 DESIGN OVERVIEW
#  2.1 ASSUMPTIONS

> 在为满足需求设计文件系统时，我们由那些既提供挑战又提供基于的假设引导。起初我们提到了一些关键观察现在我们来谈论假设的更多细节。
+ 该系统由许多廉价且容易损坏的组件建成。他必须被持久的监控，侦测，容忍和从一个远程节点上恢复错误的组件。
+ 该系统存储相当大的文件。

